= What I Learn Today 161116


== Lec3 Linear Regression의 cost 최소화 알고리즘의 원리 설명

==== What Cost(W) looks Like?


|===
|x|y
|1|1
|2|2
|3|3
|===

W = 1, Cost(W) 는 ?  0
W = 0 일 때는 4.67
W = 2 일 때는 4.67

그래프를 그리면 알지만, 기계적으로 찾아내야 한다 !

==== Gradient Descent algorithm 경사 하강 알고리즘

. 값함수를 최소화
. 많은 최소화문제에 사용됨.
. 주어진 값함수에대해서 cost(W,b)는 가장 적은 값을 찾을 것이다.
. 좀 더 일반적인 함수에도 적용이 될 것이다.

==== 어떻게 동작하는가?
. 어떤 점에서건 시작할 수 있다.
. W를 조금 바꿔서 cost(W,b) 를 줄여본다.
. 그 후의 경사를 계산해서 반복을 해본다.

==== 일반적 정의
. 경사도를 어떻게 구할 것인가? => 미분(주어진 그래프의 경사도 구함)
.

W := w - a * (a/aW) * cost(W)

=>

W := W - a/m * sig(Wxi- yi)xi

==== 미분 계산해주는 사이트
Derivative Calculator

==== Convex Function
밥그릇을 엎어놓은 ... Gradient Descent는 뭐 언제나 이런 식..
항상 답을 찾을 것이다. 언제나 그래왔듯이...


== Lab 03 Linear Regression 의 cost 최소화의 TensorFlow 구현
소스 알아서.. ㅎㅎ

== Lec 4 Multi-variable Linear Regression

==== Recap
. Hypothesis
. Cost function
. Gradient Descent Algorithm


==== Predicting Exam Score : Regression Using One Input(x)
|===
|hour|attendance|score
|10|5|90
|9|5|80
|3|2|50
|2|4|60
|11|1|40
|===


==== Hypothesis
H(x) = Wx + b
H(x1, x2) = w1*x1 + w2*x2 + b

==== Cost Function
H(x1,x2) = w1*x1 + w2*x2 + b
cost(W,b) = 1/m * sigma (H * x1, x2) - yi * pow

==== Multi-variable
H(x1,x2) = w1x1 + w2x2 + b
H(x1,x2,x3,... ,xn) = w1x1 + w2x2+ ... + wnxn

==== Matrix
위의 가로로 긴 것을 세로로 행렬로 만들어서 곱함..

https://www.mathsisfun.com/algebra/matrix-multiplying.html



==== Hypothesis without b

첫번째 가로행렬의 앞자리에는 b를 추가시키고,
세로행렬의 제일 처음에는 1 추가..


==== W vs X
배열을 같은 모양으로 맞춘 것처럼 보이게도..

==== Transpose
행렬에 T를 붙이면 역거울..

==== Hypothesis using Transpose
H(X) = WtX


== MultiVariable Linear Regression with TF
