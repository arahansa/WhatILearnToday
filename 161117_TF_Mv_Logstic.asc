= What I Learn Today 161117

== lab 04 - multi variable linear regression 실습

== 5-1: Logistic Classification의 가설 함수 정의

alpha = learning rate

Hypothesis, cost, Gradient descent

==== Classification
(Binary)
. Spam Detection : Spam or Ham
. Facebook feed : shwo or hide
. Credit Card Fraudulent Transactiton Detection : legitimate/fraud


==== 0, 1 encoding

. Malignant tumor, benign Tumor
. stock ? sell : buy


==== Pass(1) / Fail(0) based on study hours

==== Linear Regression ?
맹점 : 50시간을 공부했는데도 (합격/불합격)0 과 1로 정해지기 때문에..
선의 기울기가 달라짐. => 합격과 불합격을 구분하는 선이 달라짐.


==== Linear Regression

. We know Y is 0 or 1 ( H(x) = Wx + b)
. Hypothesis can give values large than 1 or less than 0
. 평균치보다 엄청나게 많은 녀석이 있다면...?

==== Logistic Hypothesis
H(x) = Wx + b ->
G(z) = 0 ~ 1 사이로 만들어주는 녀석이 있을까?


sigmoid : Curved in two directions like the letter 'S' or the Greek sigma

==== Logistic Hypothesis

H(x) = 1 / (1 + e pow -(WtX))


== Lecture 5-2 Logistic (regression)

. classification
. cost function
. gradient decent

==== Cost

cost(W,b) = 1/m * sigma ( Hxi-yi) pow2 when H(x) = Wx + b


==== Cost function

H(x) = Wx +b 에서

H(x) = 1 / 1+ e pow -(Wtx)

문제는 평평한 곳을 잘 못 잡을 수도 있다는 것..

* Global Minimum을 찾기가 힘들수도.. 중간에 멈춰버린다면..


==== New cost Function for logistic

cost(W) = 1/m * sum( H(x), y)
c(H(x), y) = 두가지를 나누어서 함수를 정의
 y = 1 일때는 -log(H(x))
 y = 0 일 때는 -log(1-H(x))

 줄여서

 C(H(x), y) = -ylog(H(x)) -(1-y)log(1-H(x))


 ==== Minimize Cost - Gradient descent Algorithm

 cost(W) = - 1/m sum( ylog(H(x)) + (1-y)log(1-H(x))

 W := W - a * a'/(a'W) * cost(W)



== Lec 6-1 SOftMax Regression 기본 개념

==== Multinomial classification
