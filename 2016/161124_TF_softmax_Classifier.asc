= What I Learn Today 161117

== Lab 06 - 6-1 - Softmax Regression: 기본 개념 소개 
==== sigmoid
. sigmoid = 아름다운 함수 g(z)
. x->w-> z 를 통과하면 s -> y hat 이 된다. 

==== Logistic regression

==== Multinomial Classfication
. 사실 Binary Classfication 으로도 가능하다. 두개로 계쏙 나누면 되니깐.. 
. 행렬로 해서 각각에서 시그모이드를 적용하는 것까지 얘기함...

== lab 6-2 - Softmax classifier 의 cost함수 

==== Where is sigmoid?
. softmax = y 2.0, 1.0, 0.1 을 소프트맥스에 넣게되면 0.7, 0.2, 0.1 이 된다.
. scores -> probabilities 가 된다.
. 그 중의 하나를 말해줘 one-hot-encoding. 텐서플로우에는 argmax 가 있다. 

==== cost function
. 예측한 값과 실제값이 얼마나 차이가 있는지 나타내는 cost function 
. cross - entropy
. s(y)(=y hat) =>D(S,Y)<= (L=Y)
. Cross-entropy cost function 


