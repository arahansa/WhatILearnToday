= What I Learn Today 161126

== Lec 07-01 학습 rate, Overfitting, 그리고 일반화

==== Learning rate : overshooting
. Learning rate 가 너무 크다면 ..?
. Small Learning rate 라면..?

==== Data(x) preprocessing for gradient descent
. Original data -> Zero-centered data -> normalized data


==== Overfitting
. our model is very good with training data set(with memorization)
. Not good at

==== Solutions for overfitting
. More training data !
. Reduce the number of features
. Regularization

==== Regularization
. Let's not have too big numbers in the weight

==== Summary
. Learning rate
. Data preprocessing
. Overfitting - 더 많은 데이터와 정규화


== Lec 07-02 Training/Testing 데이터 셋

==== Evaluation using training set ?

==== Training and test sets
. 자료의 앞부분을 트레이닝에 사용하고 테스트 셋은 답을 숨겨놓고 검사해봄
. Training set, Validation set, Testing

==== Online Learning
. 100 만개의 데이터가 있을 시에 10만개씩 잘라서 학습시킨다.
. 모델이 해야할일은 첫번째 했던 결과가 남아있는다.

==== Minist dataset

==== Accuracy
. 가지고 있는 실제 데이터의 Y 값을 가지고서 검사

== Lab07 learningRate, training 
